{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from utils.utils import instantiate_from_config\n",
    "from model.cldm.model import load_state_dict\n",
    "import os\n",
    "import random\n",
    "\n",
    "config_path = \"configs/v1.yaml\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "\n",
    "model = instantiate_from_config(config.model)\n",
    "model.load_state_dict(load_state_dict(\"checkpoints/epoch=32-step=124938-v1.ckpt\", \"cpu\"), strict=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import read_image, read_keypoints, draw_pose, draw_landmarks\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "prompt = \"model posing\"\n",
    "\n",
    "root = \"sample_images\"\n",
    "\n",
    "fk_jsons = os.listdir(os.path.join(root, \"fashion_keypoints_posed\"))\n",
    "hk_jsons = os.listdir(os.path.join(root, \"human_keypoints_posed\"))\n",
    "\n",
    "offsets = [0.0,30,-30]\n",
    "\n",
    "for i in fk_jsons:\n",
    "    id = i[:-7]\n",
    "    print(id)\n",
    "    for offset in offsets:\n",
    "        fk_tensor = read_keypoints(root, \"fashion_keypoints_posed\", id)\n",
    "        hk_tensor = read_keypoints(root, \"human_keypoints_posed\", id)\n",
    "    \n",
    "        blank_image = np.zeros((1024,768,3), np.uint8)\n",
    "        img,m_r,m_l = draw_pose(blank_image, hk_tensor)\n",
    "        img = draw_landmarks(img, fk_tensor, offset, m_r, m_l)\n",
    "        cv2.imwrite(root+\"/output/\"+str(id)+\"_keypoints_\"+str(offset)+\".png\", img)\n",
    "\n",
    "        batch = {\n",
    "            \"txt\": prompt,\n",
    "            \"human_image\": read_image(root+\"/human_image/\"+id+\"_0.jpg\", (512,512)),\n",
    "            \"keypoints_vis\": read_image(root+\"/output/\"+str(id)+\"_keypoints_\"+str(offset)+\".png\", (512,512)),\n",
    "            \"mask\": read_image(root+\"/cloth_agnostic_mask/\"+id+\"_0.png\", (64,64)),\n",
    "        }\n",
    "\n",
    "        from torch.utils.data import default_collate\n",
    "        for k in batch.keys():\n",
    "            batch[k] = default_collate(batch[k])\n",
    "            if isinstance(batch[k], torch.Tensor):\n",
    "                batch[k] = batch[k].unsqueeze(0).cuda()\n",
    "            else:\n",
    "                batch[k] = [batch[k]]\n",
    "        \n",
    "        images = model.log_images(batch)\n",
    "\n",
    "        for k in images:\n",
    "            if k == \"reconstruction\" or k == \"control\":\n",
    "                continue\n",
    "            images[k] = (images[k] + 1.0) / 2.0\n",
    "            batch[\"keypoints_vis\"][0] = (batch[\"keypoints_vis\"][0] + 1.0) / 2.0\n",
    "            \n",
    "            save_image(images[k], root+\"/output/\"+str(id)+\"_\"+k+\"_\"+str(offset)+\".jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fittingathome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
